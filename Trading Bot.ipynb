{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/santiago/code/trading-bot/data/USDT_BTC-historic.csv\n",
      "/Users/santiago/code/trading-bot/data/USDT_DASH-historic.csv\n",
      "/Users/santiago/code/trading-bot/data/USDT_ETH-historic.csv\n",
      "/Users/santiago/code/trading-bot/data/USDT_LTC-historic.csv\n",
      "/Users/santiago/code/trading-bot/data/USDT_STR-historic.csv\n",
      "/Users/santiago/code/trading-bot/data/USDT_XMR-historic.csv\n",
      "/Users/santiago/code/trading-bot/data/USDT_XRP-historic.csv\n"
     ]
    }
   ],
   "source": [
    "def show_csv_files_in_folder(folder, limit=10):\n",
    "    num_coins = 0\n",
    "    for dataFile in os.listdir(folder):\n",
    "        if \"csv\" not in dataFile:\n",
    "            continue\n",
    "        print(os.path.join(folder,dataFile))\n",
    "        num_coins += 1\n",
    "    return num_coins\n",
    "\n",
    "# data directory        \n",
    "folder = '/Users/santiago/code/trading-bot/data/'\n",
    "\n",
    "# list data files\n",
    "num_coins = show_csv_files_in_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '' '' '' '' '' '']\n",
      "(7, 1290, 4)\n",
      "[  1.43902080e+09   2.54382111e-01   1.28317271e+02   2.82235975e+02]\n"
     ]
    }
   ],
   "source": [
    "# load all required data\n",
    "folder = '/Users/santiago/code/trading-bot/data'\n",
    "\n",
    "# 6 coins, 1290 recordings, 4 fields (date, % change, volume, price in USD)\n",
    "data = np.ndarray(shape=(num_coins,1290,4))\n",
    "coin_mapping = np.ndarray(shape=(num_coins), dtype=np.str)\n",
    "i = 0\n",
    "for fileName in os.listdir(folder):\n",
    "    if \"csv\" not in fileName:\n",
    "        # skip non-csv files\n",
    "        continue    \n",
    "    \n",
    "    data[i] = genfromtxt(os.path.join(folder, fileName), delimiter=',')\n",
    "    coin_mapping[i] = fileName\n",
    "    i = i+1\n",
    "    \n",
    "print(coin_mapping)\n",
    "print(data.shape)\n",
    "print(data[0,0])\n",
    "# date diffClose diffVolume currentClose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "dataset shape: (1290, 42)\n",
      "labels shape: (1290, 7)\n",
      "[ 0.  0.  0.  0.  1.  0.  0.]\n",
      "valid_index = 1032\n",
      "test_index = 1161\n",
      "train_dataset shape: (1032, 42)\n",
      "train_labels shape: (1032, 7)\n",
      "valid_dataset shape: (128, 42)\n",
      "valid_labels shape: (128, 7)\n",
      "test_dataset shape: (128, 42)\n",
      "test_labels shape: (128, 7)\n",
      "not normalised input: 38.0\n",
      "[ 6.  5.  2.  6.  7.  6.  6.]\n",
      "[  2.54382104e-01  -6.81556463e-02   1.22713819e-02   9.90154967e-03\n",
      "   0.00000000e+00  -9.83441062e-03  -7.39999115e-01   0.00000000e+00\n",
      "   1.94071203e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -2.43243232e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.04555557e+02   0.00000000e+00\n",
      "   0.00000000e+00  -1.81892157e-01   0.00000000e+00   0.00000000e+00\n",
      "  -5.33366621e-01   0.00000000e+00   2.37623766e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   6.57575607e+00   0.00000000e+00\n",
      "  -1.99999854e-01   2.49999762e-01  -3.37461382e-01   0.00000000e+00\n",
      "  -6.39951766e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -3.87967765e-01   0.00000000e+00]\n",
      "not normalised labels: 0\n"
     ]
    }
   ],
   "source": [
    "# training data shape:\n",
    "# 10 prices x 6 coins\n",
    "\n",
    "# input = np.ndarray(shape=(60))\n",
    "\n",
    "# % change for 10 data points for all correncies\n",
    "# print(data[:,0:9,1].flatten())\n",
    "# USD price for all currencies for the first datapoint\n",
    "# print(data[:,0,3])\n",
    "# USD price for all currencies for the 11th datapoint\n",
    "# print(data[:,10,3])\n",
    "# % change between the first and 11th\n",
    "# performance = (data[:,10,3] / data[:,0,3]) - 1\n",
    "# print(performance)\n",
    "# print(performance - np.amax(performance))\n",
    "\n",
    "# print(np.argmax(performance))\n",
    "# perf_encoded = np.zeros(shape=(6))\n",
    "# perf_encoded[np.argmax(performance)] = 1\n",
    "# print(perf_encoded)\n",
    "\n",
    "\n",
    "# training_set = np.ndarray(shape(1280))\n",
    "\n",
    "# number of total data points\n",
    "num_data_points = data.shape[1]\n",
    "# number of currencies\n",
    "num_currencies = num_coins\n",
    "print(str(num_currencies))\n",
    "# number of data points per input\n",
    "num_datapoints_per_training_item = 6\n",
    "\n",
    "dataset = np.zeros(shape=(num_data_points, (num_currencies * num_datapoints_per_training_item)), dtype=np.float32)\n",
    "print(\"dataset shape: \" + str(dataset.shape))\n",
    "labels = np.zeros(shape=(num_data_points, num_currencies), dtype=np.float32)\n",
    "print(\"labels shape: \" + str(labels.shape))\n",
    "\n",
    "for index in range(num_data_points - num_datapoints_per_training_item - 1):\n",
    "    dataset[index] = data[:, index:index+num_datapoints_per_training_item, 1].flatten()\n",
    "#     print(dataset[index])\n",
    "\n",
    "    # old thinking, 1 hot encoding of the currency going up the most\n",
    "    temp = (data[:, index+num_datapoints_per_training_item, 3] / data[:, index, 3]) - 1\n",
    "    labels[index, np.argmax(temp)] = 1\n",
    "\n",
    "    # new thinking, all values of growth\n",
    "#     labels[index] = temp\n",
    "\n",
    "# print(dataset[0])\n",
    "print(labels[0])\n",
    "    \n",
    "valid_index = int(num_data_points * 0.8)\n",
    "print(\"valid_index = \"+str(valid_index))\n",
    "test_index = int(num_data_points * 0.9)\n",
    "print(\"test_index = \"+str(test_index))\n",
    "    \n",
    "train_dataset = dataset[:valid_index]\n",
    "train_labels = labels[:valid_index]\n",
    "print(\"train_dataset shape: \" + str(train_dataset.shape))\n",
    "print(\"train_labels shape: \" + str(train_labels.shape))\n",
    "\n",
    "valid_dataset = dataset[valid_index+1:test_index]\n",
    "valid_labels = labels[valid_index+1:test_index]\n",
    "print(\"valid_dataset shape: \" + str(valid_dataset.shape))\n",
    "print(\"valid_labels shape: \" + str(valid_labels.shape))\n",
    "\n",
    "test_dataset = dataset[test_index+1:]\n",
    "test_labels = labels[test_index+1:]\n",
    "print(\"test_dataset shape: \" + str(test_dataset.shape))\n",
    "print(\"test_labels shape: \" + str(test_labels.shape))\n",
    "\n",
    "not_normalised = 0\n",
    "\n",
    "# normalise input values to be in range (-1,1)\n",
    "# sess = tf.Session()\n",
    "# with sess.as_default():\n",
    "#     dataset = tf.nn.l2_normalize(dataset, dim=[1]).eval()\n",
    "\n",
    "wrongValuesPerCoin = np.zeros(shape=(num_currencies))\n",
    "for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[0])):\n",
    "            if ((dataset[i,j] > 1) or (dataset[i,j] < -1)):\n",
    "                not_normalised+=1\n",
    "                wrongValuesPerCoin[j%7] += 1\n",
    "#                 print(\"(i,j) = (%d, %d) coin %d\" % (i,j,j%7))\n",
    "                \n",
    "print(\"not normalised input: \" + str(np.sum(a=wrongValuesPerCoin)))\n",
    "print(wrongValuesPerCoin)\n",
    "print(dataset[0])\n",
    "\n",
    "not_normalised = 0\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels[0])):\n",
    "        if ((dataset[i,j] > 1) or (dataset[i,j] < -1)):\n",
    "                not_normalised+=1\n",
    "print(\"not normalised labels: \" + str(not_normalised))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1-hidden 384\n",
    "---------\n",
    "Bellow NN with 1 hidden layer of 384 nodes\n",
    "\n",
    "Accuracy: 70-75%\n",
    "\n",
    "Keep Prob: 0.5\n",
    "\n",
    "Days before: 3 (6 data points)\n",
    "\n",
    "Test accuracy: 65.6%\n",
    "\n",
    "\n",
    "| num_neuros | keep_prob  | accuracy |\n",
    "|------------|------------|----------|\n",
    "|   1536     | 0.7        |   65.6% |\n",
    "| 1536 | 0.6 | 68% |\n",
    "| 1280 | 0.6 | 66.4% |\n",
    "| 1024 | 0.9 | 65.6% |\n",
    "| 1024 | 0.5 | 64.1% |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num steps: 1280\n",
      ">>with keep prob of 0.3 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 202.452332\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 8.6%\n",
      "Test accuracy: 55.5%\n",
      "| 2048 | 0.3 | 55.5% |\n",
      ">>with keep prob of 0.4 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 195.130966\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 8.6%\n",
      "Test accuracy: 62.5%\n",
      "| 2048 | 0.4 | 62.5% |\n",
      ">>with keep prob of 0.5 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 82.592285\n",
      "Minibatch accuracy: 20.3%\n",
      "Validation accuracy: 10.9%\n",
      "Test accuracy: 64.1%\n",
      "| 2048 | 0.5 | 64.1% |\n",
      ">>with keep prob of 0.6 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 121.635376\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 8.6%\n",
      "Test accuracy: 61.7%\n",
      "| 2048 | 0.6 | 61.7% |\n",
      ">>with keep prob of 0.7 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 115.968872\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 8.6%\n",
      "Test accuracy: 60.9%\n",
      "| 2048 | 0.7 | 60.9% |\n",
      ">>with keep prob of 0.9 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 69.227699\n",
      "Minibatch accuracy: 21.1%\n",
      "Validation accuracy: 8.6%\n",
      "Test accuracy: 63.3%\n",
      "| 2048 | 0.9 | 63.3% |\n",
      ">>with keep prob of 0.9 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 85.968971\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 9.4%\n",
      "Test accuracy: 64.1%\n",
      "| 2048 | 0.9 | 64.1% |\n",
      ">>with keep prob of 1.0 and neurons of 2048\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 96.236565\n",
      "Minibatch accuracy: 21.1%\n",
      "Validation accuracy: 8.6%\n",
      "Test accuracy: 62.5%\n",
      "| 2048 | 1.0 | 62.5% |\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "beta_1 = 0.001\n",
    "batch_size = 128\n",
    "hidden_neurons = 2048\n",
    "\n",
    "reluGraph = tf.Graph()\n",
    "with reluGraph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_currencies * num_datapoints_per_training_item))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_currencies))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # first step + relu\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([num_currencies * num_datapoints_per_training_item, hidden_neurons]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_neurons]))\n",
    "    \n",
    "    hidden_layer = tf.nn.dropout(\n",
    "                        tf.nn.relu(\n",
    "                            tf.matmul(tf_train_dataset, hidden_weights) + \n",
    "                            hidden_biases \n",
    "                        ),\n",
    "                        keep_prob\n",
    "                    )\n",
    "    \n",
    "    # second step + relu\n",
    "    out_weights = tf.Variable(tf.truncated_normal([hidden_neurons, num_currencies]))\n",
    "    out_biases = tf.Variable(tf.zeros([num_currencies]))\n",
    "    \n",
    "    logits = tf.matmul(hidden_layer, out_weights) + out_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    loss += (beta_1*tf.nn.l2_loss(hidden_weights) +\n",
    "             beta_1*tf.nn.l2_loss(out_weights))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    valid_relu_1 = tf.nn.relu( tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases )\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu_1, out_weights) + out_biases)\n",
    "    \n",
    "    test_relu_1 = tf.nn.relu( tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases )\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu_1, out_weights) + out_biases)\n",
    "    \n",
    "    \n",
    "num_steps = 1280\n",
    "print('num steps: ' + str(num_steps))\n",
    "\n",
    "kp = 0.5\n",
    "for kp in [0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 0.9, 1.0]:\n",
    "    print(\">>with keep prob of \" + str(kp) + \" and neurons of \" + str(hidden_neurons))\n",
    "    with tf.Session(graph=reluGraph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"Initialized - \" + str(num_steps))\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels, \n",
    "            keep_prob : kp\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      temp_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "    print(\"Test accuracy: %.1f%%\" % temp_accuracy)\n",
    "    print(\"| %d | %.1f | %.1f%% |\" % (hidden_neurons, kp, temp_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3-hidden 384 and 128\n",
    "---------\n",
    "Bellow NN with 2 hidden layers\n",
    "\n",
    "Accuracy: ?\n",
    "\n",
    "Keep Prob: 0.5\n",
    "\n",
    "Days before: 3 (6 data points)\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num steps: 1280\n",
      ">>with keep prob of 0.5 and neurons of: (512, 256, 64)\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 5369.994141\n",
      "Minibatch accuracy: 18.0%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 200: nan\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 400: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 600: nan\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 800: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1200: nan\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 10.2%\n",
      "Test accuracy: 14.8%\n",
      ">>with keep prob of 0.6 and neurons of: (512, 256, 64)\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 5388.960938\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 200: nan\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 400: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 600: nan\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 800: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1200: nan\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 10.2%\n",
      "Test accuracy: 14.8%\n",
      ">>with keep prob of 0.7 and neurons of: (512, 256, 64)\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 3204.043213\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 200: inf\n",
      "Minibatch accuracy: 21.1%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 400: inf\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 600: inf\n",
      "Minibatch accuracy: 32.0%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 800: inf\n",
      "Minibatch accuracy: 20.3%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 1000: inf\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 18.0%\n",
      "Minibatch loss at step 1200: inf\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 8.6%\n",
      "Test accuracy: 3.9%\n",
      ">>with keep prob of 0.9 and neurons of: (512, 256, 64)\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 1135.678101\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 200: nan\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 400: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 600: nan\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 800: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1200: nan\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 10.2%\n",
      "Test accuracy: 14.8%\n",
      ">>with keep prob of 0.9 and neurons of: (512, 256, 64)\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 2131.693359\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 200: nan\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 400: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 600: nan\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 800: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1200: nan\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 10.2%\n",
      "Test accuracy: 14.8%\n",
      ">>with keep prob of 1.0 and neurons of: (512, 256, 64)\n",
      "Initialized - 1280\n",
      "Minibatch loss at step 0: 3693.796875\n",
      "Minibatch accuracy: 20.3%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 200: nan\n",
      "Minibatch accuracy: 1.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 400: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 600: nan\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 800: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 1200: nan\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 10.2%\n",
      "Test accuracy: 14.8%\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "beta_1 = 0.001\n",
    "batch_size = 128\n",
    "hidden_neurons = 512\n",
    "hidden_neurons_2 = 256\n",
    "hidden_neurons_3 = 64\n",
    "\n",
    "reluGraph = tf.Graph()\n",
    "with reluGraph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_currencies * num_datapoints_per_training_item))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_currencies))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    keep_prob_2 = tf.placeholder(tf.float32)\n",
    "    keep_prob_3 = tf.placeholder(tf.float32)\n",
    "\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # first step + relu\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([num_currencies * num_datapoints_per_training_item, hidden_neurons]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_neurons]))\n",
    "    \n",
    "    hidden_layer = tf.nn.dropout(\n",
    "                        tf.nn.relu(\n",
    "                            tf.matmul(tf_train_dataset, hidden_weights) + \n",
    "                            hidden_biases \n",
    "                        ),\n",
    "                        keep_prob\n",
    "                    )\n",
    "    \n",
    "    # second step + relu\n",
    "    hidden_weights_2 = tf.Variable(tf.truncated_normal([hidden_neurons, hidden_neurons_2]))\n",
    "    hidden_biases_2 = tf.Variable(tf.zeros([hidden_neurons_2]))\n",
    "    \n",
    "    hidden_layer_2 = tf.nn.dropout(\n",
    "                        tf.nn.relu(\n",
    "                            tf.matmul(hidden_layer, hidden_weights_2) + \n",
    "                            hidden_biases_2 \n",
    "                        ),\n",
    "                        keep_prob_2\n",
    "                    )\n",
    "    \n",
    "    # third step + relu\n",
    "    hidden_weights_3 = tf.Variable(tf.truncated_normal([hidden_neurons_2, hidden_neurons_3]))\n",
    "    hidden_biases_3 = tf.Variable(tf.zeros([hidden_neurons_3]))\n",
    "    \n",
    "    hidden_layer_3 = tf.nn.dropout(\n",
    "                        tf.nn.relu(\n",
    "                            tf.matmul(hidden_layer_2, hidden_weights_3) + \n",
    "                            hidden_biases_3 \n",
    "                        ),\n",
    "                        keep_prob_3\n",
    "                    )\n",
    "    \n",
    "    # fourth step + relu\n",
    "    out_weights = tf.Variable(tf.truncated_normal([hidden_neurons_3, num_currencies]))\n",
    "    out_biases = tf.Variable(tf.zeros([num_currencies]))\n",
    "    \n",
    "    logits = tf.matmul(hidden_layer_3, out_weights) + out_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    loss += (beta_1*tf.nn.l2_loss(hidden_weights) +\n",
    "             beta_1*tf.nn.l2_loss(out_weights) + \n",
    "             beta_1*tf.nn.l2_loss(hidden_weights_2) +\n",
    "             beta_1*tf.nn.l2_loss(hidden_weights_3))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    valid_relu_1 = tf.nn.relu( tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases )\n",
    "    valid_relu_2 = tf.nn.relu( tf.matmul(valid_relu_1, hidden_weights_2) + hidden_biases_2 )\n",
    "    valid_relu_3 = tf.nn.relu( tf.matmul(valid_relu_2, hidden_weights_3) + hidden_biases_3 )\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_relu_3, out_weights) + out_biases)\n",
    "    \n",
    "    test_relu_1 = tf.nn.relu( tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases )\n",
    "    test_relu_2 = tf.nn.relu( tf.matmul(test_relu_1, hidden_weights_2) + hidden_biases_2 )\n",
    "    test_relu_3 = tf.nn.relu( tf.matmul(test_relu_2, hidden_weights_3) + hidden_biases_3 )\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relu_3, out_weights) + out_biases)\n",
    "    \n",
    "    \n",
    "num_steps = 1280\n",
    "print('num steps: ' + str(num_steps))\n",
    "\n",
    "for kp in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    print(\">>with keep prob of \" + str(kp) + \" and neurons of: \" + str((hidden_neurons, hidden_neurons_2, hidden_neurons_3)))\n",
    "    with tf.Session(graph=reluGraph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print(\"Initialized - \" + str(num_steps))\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels, \n",
    "            keep_prob : kp,\n",
    "            keep_prob_2 : kp*0.8,\n",
    "            keep_prob_3 : kp*0.8*0.8\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 200 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-306-0963937c0732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "\n",
    "input = train_dataset[0:128, :]\n",
    "output = train_labels[0:128, :]\n",
    "\n",
    "hidden_layer = tf.nn.dropout(\n",
    "                    tf.nn.relu(\n",
    "                        tf.matmul(input, hidden_weights) + \n",
    "                        hidden_biases \n",
    "                    ),\n",
    "                    keep_prob\n",
    "                )\n",
    "\n",
    "hidden_layer_2 = tf.nn.dropout(\n",
    "                    tf.nn.relu(\n",
    "                        tf.matmul(hidden_layer, hidden_weights_2) + \n",
    "                        hidden_biases_2 \n",
    "                    ),\n",
    "                    keep_prob_2\n",
    "                )\n",
    "\n",
    "hidden_layer_3 = tf.nn.dropout(\n",
    "                    tf.nn.relu(\n",
    "                        tf.matmul(hidden_layer_2, hidden_weights_3) + \n",
    "                        hidden_biases_3 \n",
    "                    ),\n",
    "                    keep_prob_3\n",
    "                )\n",
    "\n",
    "logits = tf.matmul(hidden_layer_3, out_weights) + out_biases\n",
    "\n",
    "print(logits[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
